{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from create_sequences import create_sequences\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout, LayerNormalization, LeakyReLU, Input\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('eua_price_data.csv', thousands=',') \n",
    "df_all['Date'] = pd.to_datetime(df_all['Date'], format='%Y-%m-%d')  \n",
    "df_all = df_all.sort_values(by = 'Date', ascending=True).reset_index(drop = True)\n",
    "\n",
    "original_EUA = df_all['EUA'].values  \n",
    "predictors_lst = ['EUA', 'Oil', 'Coal', 'NG', 'USEU', 'S&P_clean', 'DAX']\n",
    "data = df_all[predictors_lst].values  \n",
    "dates = df_all['Date'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "sequence_length = 28\n",
    "test_data_size = sequence_length + 1\n",
    "\n",
    "test_data_scaled = data_scaled[-(test_data_size + 1):] \n",
    "test_dates = dates[-(test_data_size + 1):]\n",
    "train_data_scaled = data_scaled[:-(test_data_size + 1)]\n",
    "train_dates = dates[:-(test_data_size + 1)]\n",
    "\n",
    "X_train, y_train = create_sequences(train_data_scaled, sequence_length)\n",
    "X_test, y_test = create_sequences(test_data_scaled, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Layer Normalization: Replacing BatchNormalization with LayerNormalization for RNNs/LSTMs. Layer normalization is often more effective for sequential data as it normalizes across the features rather than the batch, avoiding issues with batch-dependent statistics in recurrent networks.\n",
    "\n",
    "1. Recurrent Dropout: Added recurrent_dropout to regularize the recurrent connections, which helps in preventing overfitting in LSTMs.\n",
    "\n",
    "1. Use of Residual Connections: Added skip (residual) connections between LSTM layers. These are particularly useful for deep RNN architectures as they help mitigate vanishing gradient issues and allow the model to learn better representations.\n",
    "\n",
    "1. Learning Rate Schedule: Implemented a learning rate scheduler to adjust the learning rate dynamically, which can improve convergence and reduce the risk of overshooting.\n",
    "\n",
    "1. Optimization: Used the AdamW optimizer instead of Adam. AdamW decouples weight decay from the learning rate, often leading to better generalization.\n",
    "\n",
    "1. Leaky ReLU for Dense Layer: Changed the activation of the final dense layer to LeakyReLU for a more flexible activation in the prediction layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer with LayerNormalization and recurrent dropout\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "               activation='tanh', recurrent_activation='sigmoid',\n",
    "               return_sequences=True, recurrent_dropout=0.2,\n",
    "               kernel_regularizer=l2(0.001)))\n",
    "model.add(LayerNormalization())\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# Second LSTM layer with residual connection, LayerNormalization, and recurrent dropout\n",
    "model.add(LSTM(128, activation='tanh', recurrent_activation='sigmoid',\n",
    "               return_sequences=True, recurrent_dropout=0.2,\n",
    "               kernel_regularizer=l2(0.001)))\n",
    "model.add(LayerNormalization())\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# Third LSTM layer (final) without returning sequences, adding residual connection\n",
    "model.add(LSTM(256, activation='tanh', recurrent_activation='sigmoid',\n",
    "               return_sequences=False, recurrent_dropout=0.2,\n",
    "               kernel_regularizer=l2(0.001)))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "# Dense layer with LeakyReLU activation for flexibility in output\n",
    "model.add(Dense(len(predictors_lst)))\n",
    "# model.add(LeakyReLU(alpha=0.1))  # LeakyReLU is more flexible for output regression\n",
    "\n",
    "# Compile the model using AdamW optimizer and a learning rate scheduler\n",
    "optimizer = AdamW(learning_rate=0.001, weight_decay=1e-5)  # AdamW improves generalization\n",
    "model.compile(optimizer=optimizer, loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./saved_model_hj_v3/best_model.keras\"\n",
    "try:\n",
    "    model.load_weights(checkpoint_path) \n",
    "    with open(os.path.join(checkpoint_path.split('/')[1],checkpoint_path.split('/')[-1].split('.')[0]), 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "except:\n",
    "\n",
    "    # Learning rate scheduler callback\n",
    "    lr_scheduler_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                monitor='val_loss', \n",
    "                                save_best_only=True,\n",
    "                                mode='min',  \n",
    "                                verbose=1)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=150, batch_size=128, validation_split=0.05,\n",
    "                        verbose=1, callbacks=[checkpoint, lr_scheduler_callback, early_stopping])# ,early_stopping]) \n",
    "    \n",
    "    with open(os.path.join(checkpoint_path.split('/')[1],checkpoint_path.split('/')[-1].split('.')[0]), 'wb') as f:\n",
    "        pickle.dump(history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path) \n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "# plt.ylim([0.01,2.5])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(X_train, verbose = 0);\n",
    "train_predictions_rescaled = scaler.inverse_transform(train_predictions)\n",
    "\n",
    "test_predictions = model.predict(X_test, verbose = 0);\n",
    "test_predictions_rescaled = scaler.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_erorrs = []\n",
    "for i in range(train_predictions_rescaled.shape[1]):\n",
    "    prediction = train_predictions[:, i]\n",
    "    ground_truth = y_train[:,i]\n",
    "    rel_error = np.mean(np.sqrt(((prediction-ground_truth)**2)))\n",
    "    print(predictors_lst[i])\n",
    "    print(rel_error)\n",
    "    rel_erorrs.append(rel_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_prediction = 30*24\n",
    "num_ensemble = 3\n",
    "\n",
    "ensemble_future_predictions = []\n",
    "for en in range(num_ensemble):\n",
    "    future_predictions = []\n",
    "    current_input = train_predictions[-sequence_length:, :]\n",
    "    for i in tqdm(range(num_of_prediction)):\n",
    "        current_input_scaled = np.reshape(current_input, (1, sequence_length, current_input.shape[1]))\n",
    "        next_prediction = model.predict(current_input_scaled, verbose = 0)\n",
    "        for j in range(train_predictions_rescaled.shape[1]):\n",
    "            next_prediction[0, j] *= (1+np.random.normal(0, rel_erorrs[j]))\n",
    "        future_predictions.append(next_prediction[0])  \n",
    "        current_input = np.concatenate([current_input[1:], [next_prediction[0]]], axis=0)\n",
    "    ensemble_future_predictions.append(scaler.inverse_transform(np.array(future_predictions)))\n",
    "\n",
    "ensemble_future_predictions= np.array(ensemble_future_predictions)\n",
    "\n",
    "\n",
    "# future_predictions_original = scaler.inverse_transform(future_predictions)\n",
    "future_dates = [pd.to_datetime(train_dates[-1]) + pd.DateOffset(days=i + 1) for i in range(num_of_prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create traces for each plot\n",
    "trace1 = go.Scatter(x=dates, y=original_EUA, mode='lines', name='Historical EUA Price', \n",
    "                    line=dict(color='blue'))\n",
    "\n",
    "trace2 = go.Scatter(x=future_dates, y=ensemble_future_predictions[0,:,0], mode='lines', \n",
    "                    name='Predicted EUA Price_1', \n",
    "                    line=dict(color='red'))\n",
    "\n",
    "trace3 = go.Scatter(x=future_dates, y=ensemble_future_predictions[1,:,0], mode='lines', \n",
    "                    name='Predicted EUA Price_2', \n",
    "                    line=dict(color='purple'))\n",
    "\n",
    "trace4 = go.Scatter(x=future_dates, y=ensemble_future_predictions[2,:,0], mode='lines', \n",
    "                    name='Predicted EUA Price_3', \n",
    "                    line=dict(color='orange'))\n",
    "\n",
    "trace5 = go.Scatter(x=train_dates[-train_predictions_rescaled.shape[0]:], \n",
    "                    y=train_predictions_rescaled[:, 0], mode='lines+markers', \n",
    "                    name='Train Predicted EUA Price', marker=dict(color='green', size=4), \n",
    "                    line=dict(color='green'))\n",
    "\n",
    "# Layout for the plot\n",
    "layout = go.Layout(\n",
    "    title='EUA Price Prediction for the Next 24 Months',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='EUA Price'),\n",
    "    legend=dict(x=0, y=1, traceorder='normal'),\n",
    "    height=600,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Create the figure with the traces\n",
    "fig = go.Figure(data=[trace1, trace2, trace3, trace4, trace5], layout=layout)\n",
    "\n",
    "# Show the figure (interactive plot)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_future_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_future_predictions = np.load('temp.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates, original_EUA, 'b', label='Historical EUA Price')  # 원본 EUA 데이터\n",
    "# plt.plot(train_dates, scaler.inverse_transform(test_data_scaled)[:, 0], 'skyblue', label='Test EUA Price')  # 테스트 데이터\n",
    "plt.plot(future_dates, ensemble_future_predictions[0, :, 0].T, 'red', linewidth = 0.1, label = 'real_1')\n",
    "plt.plot(future_dates, ensemble_future_predictions[0, :, 0].T, 'purple', linewidth = 0.1, label = 'real_2')\n",
    "plt.plot(future_dates, ensemble_future_predictions[0, :, 0].T, 'yellow', linewidth = 0.1, label = 'real_3')\n",
    "plt.plot(train_dates[-train_predictions_rescaled.shape[0]:], train_predictions_rescaled[:, 0], 'g.', marker='.', markersize=2, label='Train Predicted EUA Price')\n",
    "\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('EUA Price')\n",
    "plt.title('EUA Price Prediction for the Next 24 Months')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
