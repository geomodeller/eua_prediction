{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from generate_multivariate_samples import generate_multivariate_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('eua_price_data.csv', thousands=',') \n",
    "df_all['Date'] = pd.to_datetime(df_all['Date'], format='%Y-%m-%d')  \n",
    "df_all = df_all.sort_values(by = 'Date', ascending=True).reset_index(drop = True)\n",
    "df_all = df_all[(df_all['Date'] > pd.to_datetime('2020-11-24')) & (df_all['Date'] < pd.to_datetime('2024-10-07'))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# # Assuming df_all is your DataFrame\n",
    "# fig = px.line(df_all, x='Date', y='EUA', title='EUA Over Time')\n",
    "# # Show the figure\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curate_training_test_data import curate_training_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_models = {\n",
    "    'rf':{'model': RandomForestRegressor(),\n",
    "          'param_grid': {\n",
    "          'n_estimators': [50, 100, 200],            # Number of trees in the forest\n",
    "          'max_depth': [None, 10, 20, 30],           # Maximum depth of the tree\n",
    "          'min_samples_split': [2, 5, 10],           # Minimum number of samples required to split an internal node\n",
    "          'min_samples_leaf': [1, 2, 4],             # Minimum number of samples required to be at a leaf node\n",
    "          'bootstrap': [True, False]                 # Whether bootstrap samples are used when building trees\n",
    "                        }},\n",
    "    'knn':{'model': KNeighborsRegressor(),\n",
    "          'param_grid': {\n",
    "    'n_neighbors': [3, 5, 7, 9],      # Number of neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Weight function\n",
    "    'p': [1, 2]                        # Power parameter for the Minkowski metric (1 = Manhattan, 2 = Euclidean)\n",
    "}},\n",
    "\n",
    "\n",
    "\n",
    "    'dt':{'model': DecisionTreeRegressor(),\n",
    "            'param_grid': {\n",
    "    'max_depth': [None, 10, 20, 30],              # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],              # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],                # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}},\n",
    "\n",
    "\n",
    "\n",
    "    'lasso':{'model': Lasso(),\n",
    "            'param_grid': {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],  # Regularization strength\n",
    "    'fit_intercept': [True, False],          # Whether to calculate the intercept\n",
    "    'max_iter': [1000, 2000, 3000]           # Maximum number of iterations\n",
    "}},\n",
    "\n",
    "\n",
    "    'ridge':{'model': Ridge(),\n",
    "            'param_grid':{\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "    'fit_intercept': [True, False],                 # Whether to calculate the intercept\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'sparse_cg', 'saga'],  # Solver to use for optimization\n",
    "    'max_iter': [1000, 2000, 3000]                  # Maximum number of iterations\n",
    "}},\n",
    "\n",
    "\n",
    "    'gb':{'model': GradientBoostingRegressor(),\n",
    "            'param_grid': {\n",
    "    'n_estimators': [100, 200, 300],          # Number of boosting stages\n",
    "    'learning_rate': [0.01, 0.1, 0.2],        # Learning rate\n",
    "    'max_depth': [3, 4, 5],                   # Maximum depth of individual trees\n",
    "    'subsample': [0.8, 1.0],                  # Fraction of samples used for fitting each base learner\n",
    "    'min_samples_split': [2, 5, 10]           # Minimum number of samples required to split a node\n",
    "}},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   2.3s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.8s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.2s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.2s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.6s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   4.2s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.5s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.9s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   2.5s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.5s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.4s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.3s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.5s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.8s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.2s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.4s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.8s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.6s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   2.2s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.7s\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   8.3s\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'sklearn_models_v2'\n",
    "test_date = '2024-07-01'\n",
    "\n",
    "original_EUA = df_all['EUA'].values  \n",
    "dates = df_all['Date'].values\n",
    "last_train_date = pd.to_datetime(test_date) - pd.to_timedelta(1, unit = 'day')\n",
    "for key, values in sklearn_models.items():\n",
    "    \n",
    "    modeltype = key\n",
    "    for sequence_length in [7, 14, 28]:\n",
    "        predictors_lst = ['EUA', 'Oil', 'Coal', 'NG', 'USEU', 'S&P_clean', 'DAX']\n",
    "\n",
    "        X_train, y_train, X_test, y_test, scaler = curate_training_test_data(df_all, \n",
    "                                                                            sequence_length=sequence_length,\n",
    "                                                                            test_date = test_date,\n",
    "                                                                            predictors_lst = predictors_lst )\n",
    "\n",
    "        if os.path.isfile(f'{folder_name}/{modeltype}_bestmodel_ts_{sequence_length}.pkl'):\n",
    "            continue\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = values['model']\n",
    "\n",
    "        # Define the parameter grid to search for the best hyperparameters\n",
    "        param_grid = values['param_grid']\n",
    "\n",
    "        # Set up the GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                                   cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "        # Fit the model with grid search to the training data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best parameters and model from the grid search\n",
    "        best_params = grid_search.best_params_\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Print the best parameters\n",
    "        print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # accuracy plot\n",
    "        train_predictions = best_model.predict(X_train);\n",
    "        train_predictions_rescaled = scaler.inverse_transform(train_predictions)\n",
    "        test_predictions = best_model.predict(X_test);\n",
    "        test_predictions_rescaled = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "        ground_truth_train = scaler.inverse_transform(y_train)\n",
    "        ground_truth_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "        plt.figure(figsize = (13,13))\n",
    "        for i, feature in enumerate(predictors_lst):\n",
    "            plt.subplot(len(predictors_lst)//3 + 1 if len(predictors_lst)%3 !=0 else len(predictors_lst)//3, 3, i+1)\n",
    "            plt.scatter(ground_truth_train[:,i],train_predictions_rescaled[:,i], label = 'train')\n",
    "            plt.scatter(ground_truth_test[:,i],test_predictions_rescaled[:,i], label = 'test')\n",
    "            plt.plot([min(ground_truth_train[:,i]), max(ground_truth_train[:,i])], \n",
    "                    [min(ground_truth_train[:,i]), max(ground_truth_train[:,i])], color='red', label='1:1 Line')\n",
    "            \n",
    "            r2_train = r2_score(ground_truth_train[:,i],train_predictions_rescaled[:,i])\n",
    "            r2_test = r2_score(ground_truth_test[:,i],test_predictions_rescaled[:,i])\n",
    "            plt.title(f\"{feature} - train: {r2_train:.5f} / test: {r2_test:.5f}\")\n",
    "            plt.legend()\n",
    "            plt.grid('on')\n",
    "            plt.xlabel('ground truth')\n",
    "            plt.ylabel('prediction')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{folder_name}/{modeltype}_acc_ts_{sequence_length}.pdf')\n",
    "\n",
    "        # get RMSE:\n",
    "        rel_erorrs = []\n",
    "        for i in range(test_predictions_rescaled.shape[1]):\n",
    "            prediction = test_predictions[:, i]\n",
    "            ground_truth = y_test[:,i]\n",
    "            rel_error = np.mean(np.sqrt(((prediction-ground_truth)**2)))\n",
    "            print(predictors_lst[i])\n",
    "            print(rel_error)\n",
    "            rel_erorrs.append(rel_error)\n",
    "\n",
    "\n",
    "        num_of_prediction = 30*24\n",
    "        num_ensemble = 100\n",
    "        rel_erorrs_mat = np.array([rel_erorrs for i in range(num_ensemble)])\n",
    "        corr = df_all[predictors_lst].corr()\n",
    "        for factor in [0.5, 1.0, 1.5, 2.0, 3.0, 5.0]:\n",
    "            next_predictions = []\n",
    "            current_input = X_train[-1].flatten().reshape(1,-1)\n",
    "            current_input = np.array([current_input for i in range(num_ensemble)]).squeeze()\n",
    "            for iter_ in tqdm(range(num_of_prediction)):\n",
    "                next_prediction = best_model.predict(current_input)\n",
    "                error_p = generate_multivariate_samples(corr, n_samples=num_ensemble)\n",
    "                next_prediction = next_prediction * (1+error_p*rel_erorrs_mat*factor)\n",
    "                next_predictions.append(next_prediction)\n",
    "                current_input = np.concatenate([current_input[:,len(predictors_lst):], \n",
    "                                                next_prediction], axis=1)\n",
    "            next_predictions = np.array(next_predictions)\n",
    "            future_dates = [last_train_date + pd.DateOffset(days=i + 1) for i in range(num_of_prediction)]\n",
    "            ensemble_future_predictions = np.array([scaler.inverse_transform(next_predictions[i]) for i in range(num_of_prediction)])\n",
    "\n",
    "\n",
    "            # Calculate mean, P10, and P90 of predictions\n",
    "            mean_predictions = ensemble_future_predictions[:, :, 0].mean(axis=1)\n",
    "            P50 = np.percentile(ensemble_future_predictions[:, :, 0], 50, axis=1)\n",
    "            P10 = np.percentile(ensemble_future_predictions[:, :, 0], 10, axis=1)\n",
    "            P90 = np.percentile(ensemble_future_predictions[:, :, 0], 90, axis=1)\n",
    "\n",
    "            # Create the plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            # Plot historical EUA prices\n",
    "            plt.plot(dates, original_EUA, label='Historical EUA Price', color='blue')\n",
    "\n",
    "            # Plot all realizations\n",
    "            for realization in ensemble_future_predictions[:, :, 0].T:\n",
    "                plt.plot(future_dates, realization, color='gray', alpha=0.3)\n",
    "\n",
    "            # Plot P10 and P90 percentile predictions\n",
    "            plt.plot(future_dates, P10, label='P10 & P90', color='green', linestyle='-')\n",
    "            plt.plot(future_dates, P90, color='green', linestyle='-')\n",
    "            # Plot mean of future predictions\n",
    "            plt.plot(future_dates, P50, label='Median of Predictions', color='red')\n",
    "            plt.plot(df_all[df_all['Date']>test_date]['Date'],\n",
    "                     df_all[df_all['Date']>test_date]['EUA'],\n",
    "                     color = 'orange',\n",
    "                     marker = '*',\n",
    "                     label = 'Future EUA Price' \n",
    "                     )\n",
    "\n",
    "\n",
    "            # Customize the plot\n",
    "            plt.title('EUA Price Prediction for the Next 24 Months')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('EUA Price')\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"{folder_name}/{modeltype}_timeplot_ts_{sequence_length}_factor_{str(factor).replace('.','_')}.pdf\")\n",
    "\n",
    "\n",
    "        record = {}\n",
    "        with open(f'{folder_name}/{modeltype}_record_ts_{sequence_length}.txt', 'w') as f:\n",
    "            # report metrics\n",
    "            for i, feature in enumerate(predictors_lst):\n",
    "                r2_train = r2_score(ground_truth_train[:,i],train_predictions_rescaled[:,i])\n",
    "                r2_test = r2_score(ground_truth_test[:,i],test_predictions_rescaled[:,i])\n",
    "                mse_train = mean_squared_error(ground_truth_train[:,i],train_predictions_rescaled[:,i])\n",
    "                mse_test  = mean_squared_error(ground_truth_test[:,i],test_predictions_rescaled[:,i])\n",
    "                mae_train = mean_absolute_error(ground_truth_train[:,i],train_predictions_rescaled[:,i])\n",
    "                mae_test  = mean_absolute_error(ground_truth_test[:,i],test_predictions_rescaled[:,i])\n",
    "                f.write(f'{feature}\\n')\n",
    "                f.write(f'r2(train): {r2_train}\\n')\n",
    "                f.write(f'r2(test): {r2_test}\\n')\n",
    "                f.write(f'mse(train): {mse_train}\\n')\n",
    "                f.write(f'mse(test): {mse_test}\\n')\n",
    "                f.write(f'mae(train): {mae_train}\\n')\n",
    "                f.write(f'mae(test): {mae_test}\\n')\n",
    "                f.write('---------------------------\\n')\n",
    "                record['feature'] = {\"r2_train\":r2_train, \n",
    "                                    \"r2_test\": r2_test,\n",
    "                                    \"mse_train\": mse_train, \n",
    "                                    \"mse_test\": mse_test, \n",
    "                                    \"mae_train\": mae_train, \n",
    "                                    \"mae_test\": mae_test,}\n",
    "        # save metric as dictionary\n",
    "        with open(f'{folder_name}/{modeltype}_record_ts_{sequence_length}.pkl', 'wb') as f:\n",
    "            pickle.dump(record, f)\n",
    "        # Save the best model\n",
    "        with open(f'{folder_name}/{modeltype}_bestmodel_ts_{sequence_length}.pkl', 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "        with open(f'{folder_name}/{modeltype}_bestmodel_params_ts_{sequence_length}.pkl', 'wb') as f:\n",
    "            pickle.dump(best_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
